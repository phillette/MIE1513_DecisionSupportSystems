{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Jordan Ponn (996765781) <br>\n",
    "Course: MIE1513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# pandas:Data framework library for Python\n",
    "# sklearn: Library to perform machine learning tasks\n",
    "import pandas \n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import sklearn \n",
    "import sklearn.datasets\n",
    "import sklearn.metrics as metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BINARY = 0\n",
    "TERM_FREQUENCY = 1\n",
    "LOGISTIC_REGRESSION = 0\n",
    "NAIVE_BAYES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Directories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/resources/data/20_newsgroups/sci.med/59488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/resources/data/20_newsgroups/sci.med/59072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/resources/data/20_newsgroups/sci.med/58109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/resources/data/20_newsgroups/sci.med/59588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/resources/data/20_newsgroups/sci.med/59325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Directories\n",
       "0  /resources/data/20_newsgroups/sci.med/59488\n",
       "1  /resources/data/20_newsgroups/sci.med/59072\n",
       "2  /resources/data/20_newsgroups/sci.med/58109\n",
       "3  /resources/data/20_newsgroups/sci.med/59588\n",
       "4  /resources/data/20_newsgroups/sci.med/59325"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# create file directory for all files\n",
    "files = []\n",
    "folders = []\n",
    "for (path, dirnames, filenames) in os.walk('/resources/data/20_newsgroups'):\n",
    "    folders.extend(os.path.join(path, name) for name in dirnames)\n",
    "    files.extend(os.path.join(path, name) for name in filenames)\n",
    "    \n",
    "# putting file directories into pandas dataframw\n",
    "directorydf= pandas.DataFrame(files)\n",
    "directorydf.columns = ['Directories']\n",
    "directorydf.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create function to help encoding the targets\n",
    "def label_target (d):\n",
    "    if d.find(\"alt.atheism\") > 0 :\n",
    "      return 0\n",
    "    if d.find(\"comp.graphics\") > 0:\n",
    "      return 1\n",
    "    if d.find(\"comp.os.ms-windows.misc\") > 0:\n",
    "      return 2\n",
    "    if d.find(\"comp.sys.ibm.pc.hardware\") > 0:\n",
    "      return 3\n",
    "    if d.find(\"comp.sys.mac.hardware\") > 0:\n",
    "      return 4\n",
    "    if d.find(\"comp.windows.x\") > 0:\n",
    "      return 5\n",
    "    if d.find(\"misc.forsale\") > 0:\n",
    "      return 6\n",
    "    if d.find(\"rec.autos\") > 0:\n",
    "      return 7\n",
    "    if d.find(\"rec.motorcycles\") > 0:\n",
    "      return 8\n",
    "    if d.find(\"rec.sport.baseball\") > 0:\n",
    "      return 9\n",
    "    if d.find(\"rec.sport.hockey\") > 0:\n",
    "      return 10\n",
    "    if d.find(\"sci.crypt\") > 0:\n",
    "      return 11\n",
    "    if d.find(\"sci.electronics\") > 0:\n",
    "      return 12\n",
    "    if d.find(\"sci.med\") > 0:\n",
    "      return 13\n",
    "    if d.find(\"sci.space\") > 0:\n",
    "      return 14\n",
    "    if d.find(\"soc.religion.christian\") > 0:\n",
    "      return 15\n",
    "    if d.find(\"talk.politics.guns\") > 0:\n",
    "      return 16\n",
    "    if d.find(\"talk.politics.mideast\") > 0:\n",
    "      return 17\n",
    "    if d.find(\"talk.politics.misc\") > 0:\n",
    "      return 18\n",
    "    if d.find(\"talk.religion.misc\") > 0:\n",
    "      return 19\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/notebook/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get stopword list\n",
    "import nltk\n",
    "# download required resources\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1 files\n",
      "processed 5001 files\n",
      "processed 10001 files\n",
      "processed 15001 files\n",
      "Finished processing files!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import codecs\n",
    "counter = Counter()\n",
    "# Open the files and count the word frequency in each file in a loop and update the counter after finished processing a file\n",
    "for rownum, row in enumerate(directorydf.itertuples()):\n",
    "    with codecs.open(row.Directories,\"r\" ,encoding='utf-8', errors='ignore') as myfile:\n",
    "        # Counter modified from Lab 2 to convert strings to lower case, remove stop words, and remove single letter phrases\n",
    "        counter.update([word for word in re.findall(r'\\w+', myfile.read().lower()) if (word not in stopwords and len(word) > 1)])\n",
    "    if (rownum % 5000 == 0):\n",
    "        print(\"processed %d files\" % (rownum+1))\n",
    "print(\"Finished processing files!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create features based on word frequency\n",
    "def countWordFrequency(topk, method = BINARY):\n",
    "    np = []\n",
    "    print(\"Starting file processing...\\n\")\n",
    "    # now we had top k words, count the frequecy (binary) of these words in individual file\n",
    "    for rownum, row in enumerate(directorydf.itertuples()):\n",
    "        with codecs.open(row.Directories,\"r\" ,encoding='utf-8', errors='ignore') as myfile:\n",
    "            tempCounter = Counter([word for word in re.findall(r'\\w+', myfile.read())])\n",
    "            \n",
    "            # Create features\n",
    "            topkinDoc = 0\n",
    "            if (method == TERM_FREQUENCY):\n",
    "                termCount = sum(tempCounter.values())\n",
    "                topkinDoc = [tempCounter[word]/termCount for (word,wordCount) in topk]\n",
    "            else:       \n",
    "                # if the word appears in the doc, then 1, else \n",
    "                topkinDoc = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "                \n",
    "            # create a list for top k words with encoded target and its label\n",
    "            np.append(topkinDoc+[label_target(row.Directories)]+[row.Directories])\n",
    "            if (rownum % 5000 == 0):\n",
    "                print(\"processed %d files\" % (rownum+1))\n",
    "    \n",
    "    print(\"Finished processing files!\\n\")\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDataFrame(numCommonWords, method = BINARY):\n",
    "    # find top words occured most requent in the document\n",
    "    topk = counter.most_common(numCommonWords)\n",
    "\n",
    "    # Create dataframe of word labels and category labels\n",
    "    df = pandas.DataFrame(countWordFrequency(topk, method))\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    df.columns = dfName+['target','label']\n",
    "    \n",
    "    return df, dfName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitFeaturesFromTarget(dataframe, featureHeaderList, targetHeader = 'target'):\n",
    "    # select feature attributes from pandas and convert into numpy array\n",
    "    features_df = dataframe[featureHeaderList]\n",
    "    features = features_df.as_matrix()\n",
    "    # select target attribute from pandas and convert into numpy array\n",
    "    target_df = dataframe[targetHeader]\n",
    "    target = target_df.as_matrix()\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file processing...\n",
      "\n",
      "processed 1 files\n",
      "processed 5001 files\n",
      "processed 10001 files\n",
      "processed 15001 files\n",
      "Finished processing files!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine starting number of frequent words to use as features\n",
    "k=1000\n",
    "newsgroupdf, dfName = createDataFrame(k)\n",
    "features, target = splitFeaturesFromTarget(newsgroupdf, dfName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine effect of only using top k features\n",
    "numFeatures = [100*i for i in range(1,11)]\n",
    "def tuneFeatSel(features, target, classifier = LOGISTIC_REGRESSION):\n",
    "    featTrainAcc = []\n",
    "    featTestAcc = []\n",
    "\n",
    "    # separate datasets into training and test datasets once, no folding\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3)\n",
    "\n",
    "    for k in numFeatures:\n",
    "        clfModel = ''\n",
    "        if(classifier == NAIVE_BAYES):\n",
    "            gnb = MultinomialNB()\n",
    "            clfModel = gnb.fit(features_train[:,:k], target_train)\n",
    "        else:\n",
    "            # use log reg classifier\n",
    "            clf = LogisticRegression()\n",
    "            # train the features and target datasets and fit to a model\n",
    "            clfModel = clf.fit(features_train[:,:k], target_train)\n",
    "            # predict target with feature test set using trained model\n",
    "        target_pred = clfModel.predict(features_test[:,:k])\n",
    "\n",
    "        # Calculate accuracy score, and store it\n",
    "        featTrainAcc.append(metrics.accuracy_score(target_train, clfModel.predict(features_train[:,:k])))\n",
    "        score = metrics.accuracy_score(target_test, target_pred)\n",
    "        featTestAcc.append(score)\n",
    "        print(\"The accuracy score for using the top %d words is\"  % k, score)\n",
    "    \n",
    "    return featTrainAcc, featTestAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using the top 100 words is 0.670166666667\n",
      "The accuracy score for using the top 200 words is 0.814166666667\n",
      "The accuracy score for using the top 300 words is 0.893166666667\n",
      "The accuracy score for using the top 400 words is 0.946833333333\n",
      "The accuracy score for using the top 500 words is 0.946\n",
      "The accuracy score for using the top 600 words is 0.944333333333\n",
      "The accuracy score for using the top 700 words is 0.944833333333\n",
      "The accuracy score for using the top 800 words is 0.945166666667\n",
      "The accuracy score for using the top 900 words is 0.945333333333\n",
      "The accuracy score for using the top 1000 words is 0.9445\n"
     ]
    }
   ],
   "source": [
    "featTrainAcc, featTestAcc = tuneFeatSel(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c1c255160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the accuracy scores for each value of k\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(numFeatures, featTrainAcc, c='b')\n",
    "ax.plot(numFeatures, featTestAcc, c='r')\n",
    "plt.xticks(numFeatures)\n",
    "plt.title(\"Top K Words\")\n",
    "ax.set_ylabel('Score accuracy')\n",
    "ax.set_xlabel('K (Number of Words)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.a) Based on the results above, selecting the top 400 common words across the corpus (ignoring stop words, and words shorter than one character) will give a fairly accurate predictive model of determining categories with decent performance speed.  Accuracy appears to plateau after this point, while performance starts to suffer.  This works fairly well because the sample size of checked words is sufficiently large to be able to encompass phrases unique to each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b) To automatically determine this number, the number of top words used, k,  was varied, and the overall performance score of the resulting model was compared.  The optimal number of k was selected at a point where performance gains dropped off.  In other words, k was selected such that increasing k would only slow down performance without increasing the accuracy of the model.  As seen in the example above, all tests after 400 words have comparable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.c) The plot shows a quick convergence to the peak performance score in both the test and training sets, as number of features k is varied.  Both show this peak is reached at k=400.  The high training shows that an accurate model was able to be generated when it uses data it has seen before (the training data), while the high test score shows that the model generalizes well (ie. The model is not overfitted to the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine effect of adjusting hyperparameter C\n",
    "parameterList = [10**(i-4) for i in range(0,9)]\n",
    "def tuneHyperPara(features, target, classifier = LOGISTIC_REGRESSION):\n",
    "    hypTrainAcc = []\n",
    "    hypTestAcc = []\n",
    "\n",
    "    # separate datasets into training and test datasets once, no folding\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3)\n",
    "\n",
    "    for para in parameterList:\n",
    "        clfModel = ''\n",
    "        \n",
    "        if(classifier == NAIVE_BAYES):\n",
    "            # use Naive Bayes classifier\n",
    "            gnb = MultinomialNB(alpha = para)\n",
    "            clfModel = gnb.fit(features_train, target_train)\n",
    "        else:\n",
    "            # use log reg classifier\n",
    "            clf = LogisticRegression(C = para)\n",
    "            # train the features and target datasets and fit to a model\n",
    "            clfModel = clf.fit(features_train, target_train)\n",
    "        # predict target with feature test set using trained model\n",
    "        target_pred = clfModel.predict(features_test)\n",
    "\n",
    "        # Calculate accuracy score, and store it\n",
    "        hypTrainAcc.append(metrics.accuracy_score(target_train, clfModel.predict(features_train)))\n",
    "        score = metrics.accuracy_score(target_test, target_pred)\n",
    "        hypTestAcc.append(score)\n",
    "        print(\"The accuracy score for using hyperparameter = %f is\"  % para, score)\n",
    "\n",
    "    return hypTrainAcc, hypTestAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.912333333333\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.944666666667\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.960333333333\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.958\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.951833333333\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.9485\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.9455\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.943833333333\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.9425\n"
     ]
    }
   ],
   "source": [
    "k = 400\n",
    "hypTrainAcc, hypTestAcc = tuneHyperPara(features[:, :k], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMuIJxDnnXEb+P7BP0RiJ4ixjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c1c1ddf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the accuracy scores for each value of c\n",
    "ax2 = plt.subplot(111)\n",
    "ax2.plot(parameterList, hypTrainAcc, c='b')\n",
    "ax2.plot(parameterList, hypTestAcc, c='r')\n",
    "plt.xticks(parameterList)\n",
    "plt.title(\"Hyperparameter Tuning\")\n",
    "ax2.set_ylabel('Score accuracy')\n",
    "ax2.set_xlabel('C (Logistic Regression Parameter)')\n",
    "ax2.set_xscale('log')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a) The plot shows that model performance peaks at c = 0.01, then diverges as it increases from there.  This would indicate that changing the default paramter from c = 1 to c = 0.01 would give a slight increase in performance.  Also, any c value beyond c= 0.01 would cause the model to overfit, and generalize poorly with unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Amount of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using 10 percent of the data is 0.927\n",
      "The accuracy score for using 20 percent of the data is 0.946666666667\n",
      "The accuracy score for using 30 percent of the data is 0.952833333333\n",
      "The accuracy score for using 40 percent of the data is 0.953833333333\n",
      "The accuracy score for using 50 percent of the data is 0.954333333333\n",
      "The accuracy score for using 60 percent of the data is 0.955\n",
      "The accuracy score for using 70 percent of the data is 0.954833333333\n",
      "The accuracy score for using 80 percent of the data is 0.954833333333\n",
      "The accuracy score for using 90 percent of the data is 0.954833333333\n",
      "The accuracy score for using 100 percent of the data is 0.954833333333\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting amount of training data used\n",
    "dataAmount = [0.1*i for i in range(1,11)]\n",
    "k = 400\n",
    "c = 0.01\n",
    "amtTrainAcc = []\n",
    "amtTestAcc = []\n",
    "\n",
    "# separate datasets into training and test datasets once, no folding\n",
    "features_train, features_test, target_train, target_test = train_test_split(features[:,:k], target, test_size=0.3)\n",
    "\n",
    "for p in dataAmount:\n",
    "    dataIndex = round(len(features)*p)\n",
    "    # use log reg classifier\n",
    "    clf = LogisticRegression(C = c)\n",
    "    # train the features and target datasets and fit to a model\n",
    "    clfModel = clf.fit(features_train[:dataIndex,:], target_train[:dataIndex])\n",
    "    # predict target with feature test set using trained model\n",
    "    target_pred = clfModel.predict(features_test)\n",
    "    \n",
    "    # Calculate accuracy score, and store it\n",
    "    amtTrainAcc.append(metrics.accuracy_score(target_train[:dataIndex], clfModel.predict(features_train[:dataIndex,:])))\n",
    "    score = metrics.accuracy_score(target_test, target_pred)\n",
    "    amtTestAcc.append(score)\n",
    "    print(\"The accuracy score for using %d percent of the data is\"  % round(p*100), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEZCAYAAACAZ8KHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVogTh5mZFfL/AYR2ZiBVvsnGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bfe5f1668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the accuract scores for each data percentage value\n",
    "ax3 = plt.subplot(111)\n",
    "ax3.plot(dataAmount, amtTrainAcc, c='b')\n",
    "ax3.plot(dataAmount, amtTestAcc, c='r')\n",
    "plt.xticks(dataAmount)\n",
    "plt.title(\"Data Amount Tuning\")\n",
    "ax3.set_ylabel('Score accuracy')\n",
    "ax3.set_xlabel('% of Training Data Used (1 = 100%)')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a) The graph shows that in general, the more data available to train the model, the better.  Some runs show better performance using less data, but this is due to overfitting the model to the training data, as seen by the huge difference between the training and test accuracy scores.  Accuracy scores eventually stabalize once the model has been provided enough variety in samples to generalize a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file processing...\n",
      "\n",
      "processed 1 files\n",
      "processed 5001 files\n",
      "processed 10001 files\n",
      "processed 15001 files\n",
      "Finished processing files!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe for TF encoded vector\n",
    "k = 400\n",
    "newsgroupTfdf, dfTfName = createDataFrame(k, method = TERM_FREQUENCY)\n",
    "featuresTf, targetTf = splitFeaturesFromTarget(newsgroupTfdf, dfTfName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edu</th>\n",
       "      <th>ax</th>\n",
       "      <th>cmu</th>\n",
       "      <th>com</th>\n",
       "      <th>cs</th>\n",
       "      <th>news</th>\n",
       "      <th>srv</th>\n",
       "      <th>cantaloupe</th>\n",
       "      <th>net</th>\n",
       "      <th>message</th>\n",
       "      <th>...</th>\n",
       "      <th>colorado</th>\n",
       "      <th>children</th>\n",
       "      <th>left</th>\n",
       "      <th>opinions</th>\n",
       "      <th>away</th>\n",
       "      <th>wed</th>\n",
       "      <th>given</th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>/resources/data/20_newsgroups/sci.med/59488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   edu  ax  cmu  com  cs  news  srv  cantaloupe  net  message  \\\n",
       "0    1   0    1    0   1     1    1           1    1        0   \n",
       "\n",
       "                      ...                       colorado  children  left  \\\n",
       "0                     ...                              0         0     0   \n",
       "\n",
       "   opinions  away  wed  given  start  target  \\\n",
       "0         0     0    0      0      0      13   \n",
       "\n",
       "                                         label  \n",
       "0  /resources/data/20_newsgroups/sci.med/59488  \n",
       "\n",
       "[1 rows x 402 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroupdf[dfTfName+['target','label']].head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edu</th>\n",
       "      <th>ax</th>\n",
       "      <th>cmu</th>\n",
       "      <th>com</th>\n",
       "      <th>cs</th>\n",
       "      <th>news</th>\n",
       "      <th>srv</th>\n",
       "      <th>cantaloupe</th>\n",
       "      <th>net</th>\n",
       "      <th>message</th>\n",
       "      <th>...</th>\n",
       "      <th>colorado</th>\n",
       "      <th>children</th>\n",
       "      <th>left</th>\n",
       "      <th>opinions</th>\n",
       "      <th>away</th>\n",
       "      <th>wed</th>\n",
       "      <th>given</th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>/resources/data/20_newsgroups/sci.med/59488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        edu   ax       cmu  com        cs      news       srv  cantaloupe  \\\n",
       "0  0.007009  0.0  0.000584  0.0  0.000584  0.001168  0.000584    0.000584   \n",
       "\n",
       "        net  message                     ...                       colorado  \\\n",
       "0  0.001168      0.0                     ...                            0.0   \n",
       "\n",
       "   children  left  opinions  away  wed  given  start  target  \\\n",
       "0       0.0   0.0       0.0   0.0  0.0    0.0    0.0      13   \n",
       "\n",
       "                                         label  \n",
       "0  /resources/data/20_newsgroups/sci.med/59488  \n",
       "\n",
       "[1 rows x 402 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroupTfdf.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.a) See above for the vector representation of both binary and TF encoding (not normalized to a unit vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelList = newsgroupdf['target'].unique()\n",
    "# Cross validation with k folds\n",
    "def kFoldCVTests(features, target, hypPara, folds, ci , classifier = LOGISTIC_REGRESSION, labels=labelList, printTrialScore=False):\n",
    "    cms = {}\n",
    "    scores   = []\n",
    "    cv = StratifiedKFold(target, n_folds=folds)\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        clfModel = ''\n",
    "        \n",
    "        if(classifier == NAIVE_BAYES):\n",
    "            gnb = MultinomialNB(alpha=hypPara)\n",
    "            clfModel = gnb.fit(features[train], target[train])\n",
    "        else:     \n",
    "            # use log reg classifier\n",
    "            clf = LogisticRegression(C=hypPara)\n",
    "            # train then immediately predict the test set\n",
    "            clfModel = clf.fit(features[train], target[train])\n",
    "            \n",
    "        target_pred = clfModel.predict(features[test])\n",
    "        \n",
    "        # compute the confusion matrix on each fold, convert it to a DataFrame and stash it for later compute\n",
    "        cms[i] = pandas.DataFrame(metrics.confusion_matrix(target[test], target_pred), columns=labels, index=labels)\n",
    "        # stash the overall accuracy on the test set for the fold too\n",
    "        score = metrics.accuracy_score(target[test], target_pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "        if (printTrialScore):\n",
    "            print(\"The accuracy score for trial %d is\" % i, score)\n",
    "\n",
    "    print(\"The average of accuracy score for %d stratified folds is\" % folds,np.mean(scores))\n",
    "    print(\"The std dev of accuracy score for %d stratified folds is\" % folds,np.std(scores))\n",
    "    print(\"The std error of the mean accuracy score for %d stratified folds is\" % folds,stats.sem(scores))\n",
    "    print(\"The %d percent confidence interval range is\" % round(ci*100), st.t.interval(ci, len(scores)-1, loc=np.mean(scores), scale=st.sem(scores)))\n",
    "    \n",
    "    return cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for trial 0 is 0.9525\n",
      "The accuracy score for trial 1 is 0.955\n",
      "The accuracy score for trial 2 is 0.9545\n",
      "The accuracy score for trial 3 is 0.961\n",
      "The accuracy score for trial 4 is 0.958\n",
      "The accuracy score for trial 5 is 0.9605\n",
      "The accuracy score for trial 6 is 0.9525\n",
      "The accuracy score for trial 7 is 0.96148074037\n",
      "The accuracy score for trial 8 is 0.955477738869\n",
      "The accuracy score for trial 9 is 0.95847923962\n",
      "The average of accuracy score for 10 stratified folds is 0.956943771886\n",
      "The std dev of accuracy score for 10 stratified folds is 0.00323343234133\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.00107781078044\n",
      "The 95 percent confidence interval range is (0.95450559450888639, 0.95938194926299947)\n"
     ]
    }
   ],
   "source": [
    "folds = 10\n",
    "ci = 0.95\n",
    "# k-fold Cross Validation for Binary encoding\n",
    "binaryCms = kFoldCVTests(features[:,:k],target, 0.01, folds, ci, printTrialScore = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare with TF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Not Normalized TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.045\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.045\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.045\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.139166666667\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.488666666667\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.749166666667\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.9125\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.9415\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.943333333333\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparamter C for TF model (not normalized)\n",
    "hypTrainAccTf, hypTestAccTf = tuneHyperPara(featuresTf, targetTf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.948141970985\n",
      "The std dev of accuracy score for 10 stratified folds is 0.0038573950315\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.00128579834383\n",
      "The 95 percent confidence interval range is (0.94523329305214832, 0.95105064891883728)\n"
     ]
    }
   ],
   "source": [
    "# k-fold Cross Validation for TF encoding (No normalization)\n",
    "cTf = 10000\n",
    "tfCms = kFoldCVTests(featuresTf, targetTf, cTf, folds, ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert all datum to unit vectors to normalize for document length\n",
    "def normalizeFeatures(features):\n",
    "    normFeatures = []\n",
    "    \n",
    "    for row in features:\n",
    "        mag = np.linalg.norm(row)\n",
    "        normRow = []\n",
    "        \n",
    "        for val in row:\n",
    "            normRow.append(val/mag)\n",
    "            \n",
    "        normFeatures.append(normRow)\n",
    "    return np.array(normFeatures)\n",
    "\n",
    "# Convert feature vectors to unit vectors to normalize for document length\n",
    "normFeatures = normalizeFeatures(featuresTf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Normalized TF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.486\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.648666666667\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.7495\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.876166666667\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.941666666667\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.951333333333\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.949166666667\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.94\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.927666666667\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparameter C for TF model (normalized vectors)\n",
    "hypTrainAccTfNorm, hypTestAccTfNorm = tuneHyperPara(normFeatures, targetTf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.950242346173\n",
      "The std dev of accuracy score for 10 stratified folds is 0.0022653854874\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.000755128495801\n",
      "The 95 percent confidence interval range is (0.94853412683752003, 0.95195056550865309)\n"
     ]
    }
   ],
   "source": [
    "# k-fold Cross Validation for TF encoding (Normalized)\n",
    "cTfNorm = 10\n",
    "normTfCms = kFoldCVTests(normFeatures, targetTf, cTfNorm, folds, ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.b) The binary encoded model (c=0.01) performs slightly better than the normalized term-frequency encoded model (c=10), which performed slightly better than the non-normalized term-frequency model (c=10000).  However, the TF models require significantly more regularization (higher c values), which would be akin to increasing the polynomial order in a linear equation.\n",
    "\n",
    "The term frequency models may be performing worse in this case because they may be going to more detail than needed in order to identify the correct classification label (ie. Binary encoding may be sufficent for this domain).  This additional detail may cause the clusters less distinct.  Also, the non-normalized version needs a lot of regularization in order for it to work in order to compensate for document lengh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a.) The chosen settings are: Binary encoding of the top 400 common words as features (excluding nltk stop-words, and words shorter than 1 character), C = 0.01 and use of all training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b.) See results above for average accuracy score over 10 stratified folds, the individual scores for each trial, and the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c.) The average accuracy score is a guess as to what how well the model performs, while the 95% confidence interval indicates a range of values which indicates the true performance of the model with 95% certainty.  These are more informative than a single trial as they correct for any bias that may occur through lucky configurations in the training/test data split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d.) The train accuracy is always higher than the test accuracy.  This is because the train accuracy is using data that it has already saw, and has trained the model on, while the test accuacy is using data not yet seen by the model.  As a result, the test accuracy is testing to see how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>3</th>\n",
       "      <th>10</th>\n",
       "      <th>0</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>19</th>\n",
       "      <th>9</th>\n",
       "      <th>16</th>\n",
       "      <th>7</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>17</th>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <th>18</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>993</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>988</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>946</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>983</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>979</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>994</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>983</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>991</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>991</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>48</td>\n",
       "      <td>787</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     13   3     10   0    12   15   2    5    11   19   9    16   7    4   \\\n",
       "13  798    0     0    0    0    0    0    0    0    0    0    1    0    0   \n",
       "3     0  961    10    9    3   14    1    0    0    0    0    0    0    1   \n",
       "10    0    0  1000    0    0    0    0    0    0    0    0    0    0    0   \n",
       "0     0    0     6  993    0    0    1    0    0    0    0    0    0    0   \n",
       "12    0    0     1   10  988    0    1    0    0    0    0    0    0    0   \n",
       "15    0   28     9   14    1  946    2    0    0    0    0    0    0    0   \n",
       "2     0    0     0    7    3    0  983    2    2    0    0    0    3    0   \n",
       "5     0    0     0    0    0    0    6  991    0    0    0    0    3    0   \n",
       "11    0    0     0    0    0    0    7    5  979    4    2    0    0    0   \n",
       "19    0    0     0    0    0    0    0    0    0  994    6    0    0    0   \n",
       "9     0    0     0    0    0    0    0    0    0    8  992    0    0    0   \n",
       "16    0    1     1    0    0    0    0    0    0    0    0  990    1    2   \n",
       "7     0    1     0    4    0    1    5    1    0    0    0    0  983    3   \n",
       "4     0    5     0    0    0    1    0    0    0    0    0    0    0  991   \n",
       "6     0    1     0    0    0    0    1    0    0    0    0    0    3    1   \n",
       "17    0    0     0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "8     0    0     0    0    0    0    0    0    0    0    0    1    0    0   \n",
       "1     0    0     0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "18    0    0     0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "14   75    0     0    0    0    0    0    0    0    0    0    0    0    1   \n",
       "\n",
       "     6    17   8    1    18   14  \n",
       "13    0    0    0    0    0  201  \n",
       "3     1    0    0    0    0    0  \n",
       "10    0    0    0    0    0    0  \n",
       "0     0    0    0    0    0    0  \n",
       "12    0    0    0    0    0    0  \n",
       "15    0    0    0    0    0    0  \n",
       "2     0    0    0    0    0    0  \n",
       "5     0    0    0    0    0    0  \n",
       "11    2    0    1    0    0    0  \n",
       "19    0    0    0    0    0    0  \n",
       "9     0    0    0    0    0    0  \n",
       "16    2    0    3    0    0    0  \n",
       "7     1    0    0    0    1    0  \n",
       "4     3    0    0    0    0    0  \n",
       "6   992    0    0    0    2    0  \n",
       "17    0  997    0    0    0    0  \n",
       "8     0    0  962    3   17   17  \n",
       "1     0    0    2  991    7    0  \n",
       "18    1    0   56   48  787  108  \n",
       "14    0    0   48    1   57  818  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Panel of all test set confusion matrices for binary encoded model\n",
    "pl = pandas.Panel(binaryCms)\n",
    "cm = pl.sum(axis=0) #Sum the confusion matrices to get one view of how well the classifiers perform\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e.) Of all the topics, there appears to the most confusion over the atheism (Group 0) and misc relgion (Group 19) groupings, followed by the misc politics (Group 18) and misc relgion (Group 19) groupings.  This is caused by the overlap in vocabulary used across these domains, which is not captured within the top 400 common words used as the model features.  Additional features would be required to handle this, such as adding weights to infrequent words, or by giving stronger weights to group specific terms after analyzing and comparing terms in each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Compare with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using the top 100 words is 0.586\n",
      "The accuracy score for using the top 200 words is 0.745333333333\n",
      "The accuracy score for using the top 300 words is 0.822833333333\n",
      "The accuracy score for using the top 400 words is 0.9125\n",
      "The accuracy score for using the top 500 words is 0.913\n",
      "The accuracy score for using the top 600 words is 0.9125\n",
      "The accuracy score for using the top 700 words is 0.910833333333\n",
      "The accuracy score for using the top 800 words is 0.907333333333\n",
      "The accuracy score for using the top 900 words is 0.91\n",
      "The accuracy score for using the top 1000 words is 0.91\n"
     ]
    }
   ],
   "source": [
    "# Determine number of word features needed\n",
    "featTrainAccNb, featTestAccNb = tuneFeatSel(features, target, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.905833333333\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.907\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.908333333333\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.910166666667\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.909\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.906666666667\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.902333333333\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.852\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.804833333333\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparameter alpha for NB model using binary encoding\n",
    "hypTrainAccNb, hypTestAccNb = tuneHyperPara(features[:,:k], target, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.694833333333\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.689\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.669333333333\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.574833333333\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.259833333333\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.0503333333333\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.0436666666667\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.0436666666667\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.0436666666667\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparameter alpha for NB model using TF encoding (not normalized)\n",
    "hypTrainAccNbTf, hypTestAccNbTf = tuneHyperPara(featuresTf, targetTf, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.879333333333\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.878\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.877\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.873166666667\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.860166666667\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.828833333333\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.704666666667\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.382166666667\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.0763333333333\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparameter alpha for NB model using TF encoding (normalized)\n",
    "hypTrainAccNbTfNorm, hypTestAccNbTfNorm = tuneHyperPara(normFeatures, targetTf, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.912837293647\n",
      "The std dev of accuracy score for 10 stratified folds is 0.00588751241526\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.00196250413842\n",
      "The 95 percent confidence interval range is (0.90839780085318611, 0.91727678644046051)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation with Naive Bayes, Binary encoding\n",
    "naiveBayesCms = kFoldCVTests(features[:,:k],target, 1, folds, ci, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.87628184092\n",
      "The std dev of accuracy score for 10 stratified folds is 0.0094334607114\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.0031444869038\n",
      "The 95 percent confidence interval range is (0.86916851734788703, 0.88339516449303346)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation with Naive Bayes, TF encoding(Not normalized)\n",
    "naiveBayesTfCms = kFoldCVTests(featuresTf,targetTf, 0.0001, folds, ci, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.877581990995\n",
      "The std dev of accuracy score for 10 stratified folds is 0.00645123818771\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.00215041272924\n",
      "The 95 percent confidence interval range is (0.87271741943720749, 0.88244656255378795)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation with Naive Bayes, TF encoding(Normalized)\n",
    "naiveBayesTfCms = kFoldCVTests(normFeatures,targetTf, 0.0001, folds, ci, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>3</th>\n",
       "      <th>10</th>\n",
       "      <th>0</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>11</th>\n",
       "      <th>19</th>\n",
       "      <th>9</th>\n",
       "      <th>16</th>\n",
       "      <th>7</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>17</th>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <th>18</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>913</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>989</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>975</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>902</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>952</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>964</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>87</td>\n",
       "      <td>779</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>943</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>985</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>959</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>964</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>949</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>993</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>905</td>\n",
       "      <td>16</td>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>933</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>56</td>\n",
       "      <td>704</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>116</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     13   3    10   0    12   15   2    5    11   19   9    16   7    4    6   \\\n",
       "13  845    1    0    0    0    1    2    2    0    0    0    1    1    3    1   \n",
       "3     0  913   12   17    8   27    7    1    0    0    0    7    1    3    4   \n",
       "10    0    0  989    4    0    3    0    0    0    0    0    2    0    1    0   \n",
       "0     0    2    3  991    1    0    2    0    0    0    0    0    1    0    0   \n",
       "12    0    5    0   15  975    0    4    0    0    0    0    1    0    0    0   \n",
       "15    0   36   27   16    2  902    6    0    0    0    1    8    1    0    0   \n",
       "2     1    1    3    8    6    0  952   10    1    0    0    1    6    3    3   \n",
       "5     0    0    0    0    0    1   22  964    4    0    0    0    7    0    0   \n",
       "11    0    1    0    0    0    0   49   87  779   33   19    0    3    9   11   \n",
       "19    0    1    0    0    0    0    1    0    0  943   55    0    0    0    0   \n",
       "9     0    0    0    0    0    0    1    0    0    7  991    0    0    0    1   \n",
       "16    0    5    1    0    0    0    3    0    0    0    0  985    3    0    0   \n",
       "7     0    3    0    2    0    1    6    3    0    0    0    9  959    4   13   \n",
       "4     1   16    0    0    0    0    4    0    0    0    0    5    2  964    7   \n",
       "6     0    7    0    0    0    0    9    0    0    0    0    2   23    6  949   \n",
       "17    1    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "8     0    0    0    0    0    0    1    0    0    0    0    6    0    0    0   \n",
       "1     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "18    0    0    0    0    0    0    1    0    0    0    0    1    0    0    3   \n",
       "14  205    0    0    0    0    0    3    0    0    0    0    0    0    0    0   \n",
       "\n",
       "     17   8    1    18   14  \n",
       "13    2    0    5    1  135  \n",
       "3     0    0    0    0    0  \n",
       "10    0    0    0    1    0  \n",
       "0     0    0    0    0    0  \n",
       "12    0    0    0    0    0  \n",
       "15    0    0    0    1    0  \n",
       "2     0    1    0    4    0  \n",
       "5     0    1    0    1    0  \n",
       "11    0    9    0    0    0  \n",
       "19    0    0    0    0    0  \n",
       "9     0    0    0    0    0  \n",
       "16    0    1    0    2    0  \n",
       "7     0    0    0    0    0  \n",
       "4     0    0    0    1    0  \n",
       "6     0    0    0    3    1  \n",
       "17  993    0    3    0    0  \n",
       "8     0  905   16   69    3  \n",
       "1     0   24  933   43    0  \n",
       "18    0  124   56  704  111  \n",
       "14    4   50    4  116  618  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Panel of all test set confusion matrices for binary encoded model as it had the best performance\n",
    "pl = pandas.Panel(naiveBayesCms)\n",
    "cm = pl.sum(axis=0) #Sum the confusion matrices to get one view of how well the classifiers perform\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6a.) Using the binary encoding, the Naive Bayes classifiers sees worse performance compared to the logistic regression models, but it is significantly faster in terms of speed. Naive Bayes is a simpler classification method which calculates a probability, while the logistic regression uses a threshold function to maps input variables to boolean outputs.  The former is less accurate as it assumes that features are independant, which is not completly true.  However, it is a close enough approximation that, given training data which accounts for this bias, can still perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIE1513 - Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert target values from multi-regression (ie. multiple classes) to binary label scheme\n",
    "# Create function to help convert encodings of the targets\n",
    "def multiTargetToBinary (val):\n",
    "    # Performing classification on Group 14 - sci.space\n",
    "    if val == 14:\n",
    "      return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binaryTarget = []\n",
    "for val in target:\n",
    "    binaryTarget.append(multiTargetToBinary(val))\n",
    "binaryTarget = np.array(binaryTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using the top 100 words is 0.953166666667\n",
      "The accuracy score for using the top 200 words is 0.996833333333\n",
      "The accuracy score for using the top 300 words is 0.997166666667\n",
      "The accuracy score for using the top 400 words is 0.999166666667\n",
      "The accuracy score for using the top 500 words is 0.998833333333\n",
      "The accuracy score for using the top 600 words is 0.998666666667\n",
      "The accuracy score for using the top 700 words is 0.998833333333\n",
      "The accuracy score for using the top 800 words is 0.998833333333\n",
      "The accuracy score for using the top 900 words is 0.999333333333\n",
      "The accuracy score for using the top 1000 words is 0.9995\n"
     ]
    }
   ],
   "source": [
    "# Determine number of words to use\n",
    "featTrainAccBinary, featTestAccBinary = tuneFeatSel(features, binaryTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c1c149588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the accuracy scores for each value of k\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(numFeatures, featTrainAccBinary, c='b')\n",
    "ax.plot(numFeatures, featTestAccBinary, c='r')\n",
    "plt.xticks(numFeatures)\n",
    "plt.title(\"Top K Words\")\n",
    "ax.set_ylabel('Score accuracy')\n",
    "ax.set_xlabel('K (Number of Words)')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1) The two 200 common words are used as features.  This is significantly lower than the multiclass model as the classification requirements in this case are much simpler - instead of identifying among 20 classes, we only need to determine of a document belongs in one specific class or not (sci.space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2) See answer in Multi-class regression for automated way to select k features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3) There is a significant increase in model performance until the word count reaches 200.  After this point, the performance plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.954\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.954\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.9885\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.996666666667\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.996833333333\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.996333333333\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.996333333333\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.995833333333\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.995666666667\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparameter c for Binary Logistic Regression model using binary encoding\n",
    "k = 200\n",
    "hypTrainAccBinary, hypTestAccBinary = tuneHyperPara(features[:,:k], binaryTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOIGYmVlenEDMzCwvTiBmZpaX/w/KuPGXVnxduAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bfe65d160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the accuracy scores for each value of c\n",
    "ax2 = plt.subplot(111)\n",
    "ax2.plot(parameterList, hypTrainAccBinary, c='b')\n",
    "ax2.plot(parameterList, hypTestAccBinary, c='r')\n",
    "plt.xticks(parameterList)\n",
    "plt.title(\"Hyperparameter Tuning\")\n",
    "ax2.set_ylabel('Score accuracy')\n",
    "ax2.set_xlabel('C (Logistic Regression Parameter)')\n",
    "ax2.set_xscale('log')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) Performance stays constant until after c=0.001, at which point, it rapidly increases until it starts to plateau at c=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1) See answer in Multi-class regression for improved performance with increase in training data volune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1) The vector representations are the same as the multi-class regression.  The only difference is the target labels, which were converted from a 20-label solution space to a binary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for trial 0 is 0.9955\n",
      "The accuracy score for trial 1 is 0.9985\n",
      "The accuracy score for trial 2 is 0.986\n",
      "The accuracy score for trial 3 is 0.9995\n",
      "The accuracy score for trial 4 is 0.989\n",
      "The accuracy score for trial 5 is 1.0\n",
      "The accuracy score for trial 6 is 0.9985\n",
      "The accuracy score for trial 7 is 0.99899949975\n",
      "The accuracy score for trial 8 is 0.999499749875\n",
      "The accuracy score for trial 9 is 0.99899949975\n",
      "The average of accuracy score for 10 stratified folds is 0.996449874937\n",
      "The std dev of accuracy score for 10 stratified folds is 0.00467138493482\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.00155712831161\n",
      "The 95 percent confidence interval range is (0.99292740597406204, 0.99997234390087542)\n"
     ]
    }
   ],
   "source": [
    "# k-folds cv, binary encoding\n",
    "binaryCms = kFoldCVTests(features[:,:k],binaryTarget, 0.1, folds, ci, labels = [0,1], printTrialScore = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.955833333333\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.955833333333\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.955833333333\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.955833333333\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.955833333333\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.956\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.970833333333\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.988333333333\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.994666666667\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparameter c for Binary Logistic Regression model using tf encoding (no normalizing)\n",
    "hypTrainAccBinaryTf, hypTestAccBinaryTf = tuneHyperPara(featuresTf[:,:k], binaryTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.947833333333\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.947833333333\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.947833333333\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.9565\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.980333333333\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.992\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.995\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.994666666667\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.993833333333\n"
     ]
    }
   ],
   "source": [
    "# Determine effect of adjusting hyperparameter C for TF model (normalized vectors)\n",
    "hypTrainAccBinaryTfNorm, hypTestAccBinaryTfNorm = tuneHyperPara(normFeatures[:,:k], binaryTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.990849124562\n",
      "The std dev of accuracy score for 10 stratified folds is 0.00816989055602\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.00272329685201\n",
      "The 95 percent confidence interval range is (0.98468859908224071, 0.99700965004232134)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cv, tf encoding (not normalized)\n",
    "binaryTfCms = kFoldCVTests(featuresTf[:,:k],binaryTarget, 10000, folds, ci, labels = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.99099929965\n",
      "The std dev of accuracy score for 10 stratified folds is 0.00930025530416\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.00310008510139\n",
      "The 95 percent confidence interval range is (0.98398641993261715, 0.99801217936703246)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cv, tf encoding (normalized)\n",
    "binaryTfCms = kFoldCVTests(normFeatures[:,:k],binaryTarget, 100, folds, ci, labels = [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5b.) Similar to the multi-class model, the binary encoded features performed better than the term frequency ones.  Again, with a simpler classification need, the detail required in the features to correctly identify the classes is significantly reduced.  Overall, TF encoding blur the lines between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using the top 100 words is 0.9465\n",
      "The accuracy score for using the top 200 words is 0.980333333333\n",
      "The accuracy score for using the top 300 words is 0.976\n",
      "The accuracy score for using the top 400 words is 0.991666666667\n",
      "The accuracy score for using the top 500 words is 0.989666666667\n",
      "The accuracy score for using the top 600 words is 0.989833333333\n",
      "The accuracy score for using the top 700 words is 0.988166666667\n",
      "The accuracy score for using the top 800 words is 0.989\n",
      "The accuracy score for using the top 900 words is 0.989666666667\n",
      "The accuracy score for using the top 1000 words is 0.99\n"
     ]
    }
   ],
   "source": [
    "# Compare with Naive Bayes\n",
    "featTrainAccBinaryBn, featTestAccBinaryNb = tuneFeatSel(features, binaryTarget, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for using hyperparameter = 0.000100 is 0.994\n",
      "The accuracy score for using hyperparameter = 0.001000 is 0.994\n",
      "The accuracy score for using hyperparameter = 0.010000 is 0.994\n",
      "The accuracy score for using hyperparameter = 0.100000 is 0.994166666667\n",
      "The accuracy score for using hyperparameter = 1.000000 is 0.9935\n",
      "The accuracy score for using hyperparameter = 10.000000 is 0.9885\n",
      "The accuracy score for using hyperparameter = 100.000000 is 0.9445\n",
      "The accuracy score for using hyperparameter = 1000.000000 is 0.947166666667\n",
      "The accuracy score for using hyperparameter = 10000.000000 is 0.949\n"
     ]
    }
   ],
   "source": [
    "k = 400\n",
    "# Determine effect of adjusting hyperparameter alpha for NB model using binary encoding\n",
    "hypTrainAccBinaryNb, hypTestAccBinaryNb = tuneHyperPara(features[:,:k], binaryTarget, NAIVE_BAYES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of accuracy score for 10 stratified folds is 0.967099074537\n",
      "The std dev of accuracy score for 10 stratified folds is 0.0648378071637\n",
      "The std error of the mean accuracy score for 10 stratified folds is 0.0216126023879\n",
      "The 95 percent confidence interval range is (0.91820797124001663, 1.0159901778345208)\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross validation of Naive Bayes with Binary encoding\n",
    "naiveBayesBinaryCms = kFoldCVTests(features[:,:k],binaryTarget, 0.0001, folds, ci, NAIVE_BAYES, labels = [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.a) Similar to the multi-classifier, the Naive Bayes performs worse compared to the logistic regression model.  This is due to the error in calculating the classification probabilities where each feature is assumed to be independant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a.) The chosen settings are: Binary encoding of the top 200 common words as features, C = 0.1 and use of all training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b.) See above for all trial runs, average score, and 95% confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c.) See answer for multi-classification example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d.) See answer for multi-classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18937</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1\n",
       "0  18937   60\n",
       "1     11  989"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Panel of all test set confusion matrices for binary encoded model\n",
    "pl = pandas.Panel(binaryCms)\n",
    "cm = pl.sum(axis=0) #Sum the confusion matrices to get one view of how well the classifiers perform\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e.) There are only a handful of cases which were not classified correctly.  These documents are either missing phrases used in the top common words, or require additional weighting for group specific terms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
